# ==========================================
# STEP 1: Install dependencies
# ==========================================
!pip install scikit-learn matplotlib seaborn pandas numpy

# ==========================================
# STEP 2: Import libraries
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# ==========================================
# STEP 3: Upload CSV
# ==========================================
from google.colab import files
uploaded = files.upload()

# Detect uploaded filename
filename = list(uploaded.keys())[0]
print("üìÇ Using file:", filename)

# Load dataset
df = pd.read_csv(filename)
print("‚úÖ Dataset loaded. Shape:", df.shape)
print(df.head())

# ==========================================
# STEP 4: Check columns
# ==========================================
print("\nüîé Columns in dataset:")
print(df.columns)

# ==========================================
# STEP 5: Define target column automatically
# ==========================================
target_col = None

# Kaggle dataset: StudentsPerformance.csv
if set(["math score","reading score","writing score"]).issubset(df.columns):
    df["avg_score"] = df[["math score","reading score","writing score"]].mean(axis=1)
    target_col = "avg_score"

elif set(["math_score","reading_score","writing_score"]).issubset(df.columns):
    df["avg_score"] = df[["math_score","reading_score","writing_score"]].mean(axis=1)
    target_col = "avg_score"

# UCI dataset: student-mat.csv
elif "G3" in df.columns:
    df = df.drop(columns=["G1","G2"], errors="ignore")  # prevent leakage
    target_col = "G3"

if not target_col:
    raise ValueError("‚ùå Could not detect target column. Please check dataset.")

print("üéØ Target column:", target_col)

# ==========================================
# STEP 6: Basic Data Exploration
# ==========================================
print(df.info())
print(df.isna().sum())
print(df.describe(include="all"))

sns.histplot(df[target_col], bins=20, kde=True)
plt.title("Distribution of Target Scores")
plt.show()

# Correlation heatmap for numeric columns
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Example: boxplot by gender if exists
if "gender" in df.columns:
    sns.boxplot(x="gender", y=target_col, data=df)
    plt.title("Performance by Gender")
    plt.show()

# ==========================================
# STEP 7: Preprocessing
# ==========================================
X = df.drop(columns=[target_col])
y = df[target_col]

numeric_features = X.select_dtypes(include=["int64","float64"]).columns
categorical_features = X.select_dtypes(include=["object"]).columns

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# ==========================================
# STEP 8: Train-test split
# ==========================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ==========================================
# STEP 9: Linear Regression
# ==========================================
lr_model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", LinearRegression())
])

lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

print("\nüìä Linear Regression Results:")
print("R2:", r2_score(y_test, y_pred_lr))

rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))   # FIXED ‚úÖ
print("RMSE:", rmse)

print("MAE:", mean_absolute_error(y_test, y_pred_lr))

# ==========================================
# STEP 10: Random Forest with GridSearch
# ==========================================
rf_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", RandomForestRegressor(random_state=42))
])

param_grid = {
    "regressor__n_estimators": [100, 200],
    "regressor__max_depth": [None, 10],
    "regressor__min_samples_split": [2, 5]
}

grid_search = GridSearchCV(rf_pipeline, param_grid, cv=3, n_jobs=-1, scoring="r2")
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
y_pred_rf = best_model.predict(X_test)

print("\nüå≤ Random Forest Results:")
print("Best Params:", grid_search.best_params_)
print("R2:", r2_score(y_test, y_pred_rf))

rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))   # FIXED ‚úÖ
print("RMSE:", rmse_rf)

print("MAE:", mean_absolute_error(y_test, y_pred_rf))

# ==========================================
# STEP 11: Feature Importance
# ==========================================
# Get feature names
encoder = best_model.named_steps["preprocessor"].named_transformers_["cat"].named_steps["onehot"]
encoded_cat_features = encoder.get_feature_names_out(categorical_features)
all_features = list(numeric_features) + list(encoded_cat_features)

importances = best_model.named_steps["regressor"].feature_importances_
feat_imp = pd.DataFrame({"feature": all_features, "importance": importances})
feat_imp = feat_imp.sort_values(by="importance", ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x="importance", y="feature", data=feat_imp.head(15))
plt.title("Top 15 Feature Importances")
plt.show()
